# Enhancing Large Language Modelsâ€™ Reasoning Capabilities through Contrastive Prompting (CP) and Direct Preference Optimization (DPO)

**Authors: Altria Wang (zw2958), Kevin Tang (kt2942)**

In this project, we explore the effect of using Contrastive Prompting (CP) and Direct Preference Optimization to enhance the reasoning capability of Llama3-8B-instruct model. Detailed experimental setup is specified in our final project report, which is also included in this repository.

This repository contains the data and code to generate preference datasets and perform data preprocessing. All raw data were obtained from this [repo](https://github.com/yao8839836/cp) (original study on Contrastive Prompting).

## Repository Structure
- **datasets**: contains the original data for the six selected reasoning tasks
- **data**: contains the train, val, and test splits for all datasets
- **llm_output**: contains the datasets combined with generated response from Llama3-8B-instruct (CP)
- **preference_data**: contains the processed preference datasets
- ***.py**: functional codes for data preprocessing and LLM response generation

## Computational Resource Requirements
We recommend running the code in this repo on a GCP VM that has the following configurations:

- **Region**: us-east1
- **Device Type**: G2 with 1 L4 GPU
- **Persistent Disk Size**: 200GB
- **OS**: Deep Learning VM for PyTorch 2.4 with CUDA 12.4 M126

You would also need to install the `transformers` library and all its dependency on the VM to successfully run inference for Llama3.

## Execution Instructions
1. Clone this repo onto your VM
2. Run `data_preprocessing.py` on all datasets to generates the datasets with LLM generated responses.
    ```
    python3 data_preprocessing.py aqua strategy_qa coin_flip object_tracking last_letter bigbench_date
    ```
    You can also select specific dataset(s) to process by only specifying its name.
3. Run `generate_preference_dataset.py` on all datasets to generate the preference datasets which will be used for DPO fine-tuning:
    ```
    python3 generate_preference_datasets.py
    ```
    To run this code for specific datasets, comment out the function invocation for other datasets in the main section of this code.
4. Run `calculate_accuracy.py` to get the test accuracies on all six reasoning tasks using Zero-Shot prompting:
    ```
    python3 calculate_accuracy.py aqua strategy_qa coin_flip object_tracking last_letter bigbench_date
    ```
    You can also select specific dataset(s) to process by only specifying its name.
5. Run `analysis.py` to calculate the precision for the generated responses on train and val sets using CP:
    ```
    python3 analysis.py all
    ```
    You can also select specific dataset(s) to process by only specifying its name.

Finally, you would need to use all data under `preference_data` in the DPO stage to fine-tune the model.

## Results

### Tests statistics for CP + DPO fine-tuning

The following table shows the accuracy scores of all experiments conducted in this project, regarding the six datasets we selected. All tests were conducted using the Llama3-8B-instruct model. We calculated accuracies of responses generated by the standard Llama3-8B-instruct model and the model fine-tuned with CP+DPO.


|              | Zero-Shot (baseline)    | Zero-Shot-CP-DPO |
|--------------|----------------------------|-----------------|
| AQuA | 0.39 | NA |
| Strategy-QA | 0.66 | NA |
| Coin Flip | 0.46 | NA |
| Object Tracking | 0.29| NA |
| Last Letters | 0.14 | NA |
| BigBench Date | 0.43 | NA |


### Analysis on train and val preference datasets

The following tables demonstrates the precision for correct and wrong answer generated using Contrastive Prompting. We collected these data to investigate how accurate CP is and what kind of impact it has on the final fine-runed model.

**Precision on generated answers in training sets**

|              | Correct Precision   | Wrong Precision |
|--------------|---------------------|-----------------|
| AQuA | 0.29 | 0.77 |
| Strategy-QA | 0.63 | 0.63 |
| Coin Flip | 0.46 | 0.46 |
| Object Tracking | 0.01 | 0.99 |
| Last Letters | 0.50 | 0.92 |
| BigBench Date | 0.30 | 0.67 |

**Precision on generated answers in validation sets**

|              | Correct Precision   | Wrong Precision |
|--------------|---------------------|-----------------|
| AQuA | 0.27 | 0.82 |
| Strategy-QA | 0.66 | 0.66 |
| Coin Flip | 0.46 | 0.46 |
| Object Tracking | 0.01 | 1.00 |
| Last Letters | 0.45 | 0.97 |
| BigBench Date | 0.28 | 0.64 |

We can derive several insights from the precision compar-
isons. First, the precision for the generated correct answers
is relatively low across all tasks, with the highest score
of 0.63 observed on the Strategy QA training set (assume
we only compare training sets here). Notably, the model
performed extremely poorly on the Last Letter task, achieving
a correct precision close to zero, which indicates that very
few questions were interpreted or solved correctly. Addition-
ally, the model demonstrated limited capabilities in solving
mathematical problems. All such undesired behaviors likely
due to the constrained model size of LLaMA3-8B-Instruct,
since the experiment result shown by Yan et al. are relatively
higher when using GPT-4.

The relatively low precision on correct responses suggests
that fine-tuning with such data may mislead the model by rein-
forcing suboptimal patterns, potentially degrading its overall
performance. However, a key observation is the consistency
between Table II (training precision) and Table III (validation
precision), which indicates minimal distributional differences
between the training and validation datasets. This stability
could contribute to improved generalization and better per-
formance of the fine-tuned model.
